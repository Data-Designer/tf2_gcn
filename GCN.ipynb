{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "import scipy.sparse as sp\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "\n",
    "config = {\n",
    "    'dataset': 'cora',\n",
    "    'hidden1': 16,\n",
    "    'epochs': 200,\n",
    "    'early_stopping': 20,\n",
    "    'weight_decay': 5e-4,\n",
    "    'learning_rate': 0.01,\n",
    "    'dropout': 0.,\n",
    "    'verbose': False,\n",
    "    'logging': False,\n",
    "    'gpu_id': None\n",
    "}\n",
    "\n",
    "FLAGS = EasyDict(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 辅助函数\n",
    "\n",
    "## 数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_planetoid(dataset):\n",
    "    keys = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = defaultdict()\n",
    "    for key in keys:\n",
    "        with open('data_split/ind.{}.{}'.format(dataset, key), 'rb') as f:\n",
    "            objects[key] = pickle.load(f, encoding='latin1')\n",
    "    test_index = [int(x) for x in open('data_split/ind.{}.test.index'.format(dataset))]\n",
    "    test_index_sort = np.sort(test_index)\n",
    "    G = nx.from_dict_of_lists(objects['graph'])\n",
    "\n",
    "    A_mat = nx.adjacency_matrix(G)\n",
    "    X_mat = sp.vstack((objects['allx'], objects['tx'])).tolil()\n",
    "    X_mat[test_index, :] = X_mat[test_index_sort, :]\n",
    "    z_vec = np.vstack((objects['ally'], objects['ty']))\n",
    "    z_vec[test_index, :] = z_vec[test_index_sort, :]\n",
    "    z_vec = z_vec.argmax(1)\n",
    "\n",
    "    train_idx = range(len(objects['y']))\n",
    "    val_idx = range(len(objects['y']), len(objects['y']) + 500)\n",
    "    test_idx = test_index_sort.tolist()\n",
    "\n",
    "    return A_mat, X_mat, z_vec, train_idx, val_idx, test_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用于处理稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 稀疏矩阵的 dropout\n",
    "def sparse_dropout(x, dropout_rate, noise_shape):\n",
    "    random_tensor = 1 - dropout_rate\n",
    "    random_tensor += tf.random.uniform(noise_shape)\n",
    "    dropout_mask = tf.cast(tf.floor(random_tensor), dtype=tf.bool)\n",
    "    # 从稀疏矩阵中取出dropout_mask对应的元素\n",
    "    pre_out = tf.sparse.retain(x, dropout_mask)\n",
    "    return pre_out * (1. / (1 - dropout_rate))\n",
    "\n",
    "# 稀疏矩阵转稀疏张量\n",
    "def sp_matrix_to_sp_tensor(M):\n",
    "    if not isinstance(M, sp.csr.csr_matrix):\n",
    "        M = M.tocsr()\n",
    "    # 获取非0元素坐标\n",
    "    row, col = M.nonzero()\n",
    "    # SparseTensor参数：二维坐标数组，数据，形状\n",
    "    X = tf.SparseTensor(np.mat([row, col]).T, M.data, M.shape)\n",
    "    X = tf.cast(X, tf.float32)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义图卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations, regularizers, constraints, initializers\n",
    "\n",
    "class GCNConv(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                 units,\n",
    "                 activation=lambda x: x,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(GCNConv, self).__init__()\n",
    "\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.use_bias = use_bias\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\" GCN has two inputs : [shape(An), shape(X)]\n",
    "        \"\"\"\n",
    "        fdim = input_shape[1][1]  # feature dim\n",
    "        # 初始化权重矩阵\n",
    "        self.weight = self.add_weight(name=\"weight\",\n",
    "                                      shape=(fdim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      trainable=True)\n",
    "        if self.use_bias:\n",
    "            # 初始化偏置项\n",
    "            self.bias = self.add_weight(name=\"bias\",\n",
    "                                        shape=(self.units, ),\n",
    "                                        initializer=self.bias_initializer,\n",
    "                                        trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\" GCN has two inputs : [An, X]\n",
    "        \"\"\"\n",
    "        self.An = inputs[0]\n",
    "        self.X = inputs[1]\n",
    "        # 计算 XW\n",
    "        if isinstance(self.X, tf.SparseTensor):\n",
    "            h = tf.sparse.sparse_dense_matmul(self.X, self.weight)\n",
    "        else:\n",
    "            h = tf.matmul(self.X, self.weight)\n",
    "        # 计算 AXW\n",
    "        output = tf.sparse.sparse_dense_matmul(self.An, h)\n",
    "\n",
    "        if self.use_bias:\n",
    "            output = tf.nn.bias_add(output, self.bias)\n",
    "\n",
    "        if self.activation:\n",
    "            output = self.activation(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义GCN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "class GCN():\n",
    "    def __init__(self, An, X, sizes, **kwargs):\n",
    "        self.with_relu = True\n",
    "        self.with_bias = True\n",
    "\n",
    "        self.lr = FLAGS.learning_rate\n",
    "        self.dropout = FLAGS.dropout\n",
    "        self.verbose = FLAGS.verbose\n",
    "        \n",
    "        self.An = An\n",
    "        self.X = X\n",
    "        self.layer_sizes = sizes\n",
    "        self.shape = An.shape\n",
    "\n",
    "        self.An_tf = sp_matrix_to_sp_tensor(self.An)\n",
    "        self.X_tf = sp_matrix_to_sp_tensor(self.X)\n",
    "\n",
    "        self.layer1 = GCNConv(self.layer_sizes[0], activation='relu')\n",
    "        self.layer2 = GCNConv(self.layer_sizes[1])\n",
    "        self.opt = tf.optimizers.Adam(learning_rate=self.lr)\n",
    "\n",
    "    def train(self, idx_train, labels_train, idx_val, labels_val):\n",
    "        K = labels_train.max() + 1\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        # use adam to optimize\n",
    "        for it in range(FLAGS.epochs):\n",
    "            tic = time()\n",
    "            with tf.GradientTape() as tape:\n",
    "                _loss = self.loss_fn(idx_train, np.eye(K)[labels_train])\n",
    "\n",
    "            # optimize over weights\n",
    "            grad_list = tape.gradient(_loss, self.var_list)\n",
    "            grads_and_vars = zip(grad_list, self.var_list)\n",
    "            self.opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "            # evaluate on the training\n",
    "            train_loss, train_acc = self.evaluate(idx_train, labels_train, training=True)\n",
    "            train_losses.append(train_loss)\n",
    "            val_loss, val_acc = self.evaluate(idx_val, labels_val, training=False)\n",
    "            val_losses.append(val_loss)\n",
    "            toc = time()\n",
    "            if self.verbose:\n",
    "                print(\"iter:{:03d}\".format(it),\n",
    "                      \"train_loss:{:.4f}\".format(train_loss),\n",
    "                      \"train_acc:{:.4f}\".format(train_acc),\n",
    "                      \"val_loss:{:.4f}\".format(val_loss),\n",
    "                      \"val_acc:{:.4f}\".format(val_acc),\n",
    "                      \"time:{:.4f}\".format(toc - tic))\n",
    "        return train_losses\n",
    "\n",
    "    def loss_fn(self, idx, labels, training=True):\n",
    "        if training:\n",
    "            # .nnz 是获得X中元素的个数\n",
    "            _X = sparse_dropout(self.X_tf, self.dropout, [self.X.nnz])\n",
    "        else:\n",
    "            _X = self.X_tf\n",
    "\n",
    "        self.h1 = self.layer1([self.An_tf, _X])\n",
    "        if training:\n",
    "            _h1 = tf.nn.dropout(self.h1, self.dropout)\n",
    "        else:\n",
    "            _h1 = self.h1\n",
    "\n",
    "        self.h2 = self.layer2([self.An_tf, _h1])\n",
    "        self.var_list = self.layer1.weights + self.layer2.weights\n",
    "        # calculate the loss base on idx and labels\n",
    "        _logits = tf.gather(self.h2, idx)\n",
    "        _loss_per_node = tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                 logits=_logits)\n",
    "        _loss = tf.reduce_mean(_loss_per_node)\n",
    "        # 加上 l2 正则化项\n",
    "        _loss += FLAGS.weight_decay * sum(map(tf.nn.l2_loss, self.layer1.weights))\n",
    "        return _loss\n",
    "\n",
    "    def evaluate(self, idx, true_labels, training):\n",
    "        K = true_labels.max() + 1\n",
    "        _loss = self.loss_fn(idx, np.eye(K)[true_labels], training=training).numpy()\n",
    "        _pred_logits = tf.gather(self.h2, idx)\n",
    "        _pred_labels = tf.argmax(_pred_logits, axis=1).numpy()\n",
    "        _acc = accuracy_score(_pred_labels, true_labels)\n",
    "        return _loss, _acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset cora Test loss 0.6368 test acc 0.8060\n"
     ]
    }
   ],
   "source": [
    "# 计算标准化的邻接矩阵：根号D * A * 根号D\n",
    "def preprocess_graph(adj):\n",
    "    # _A = A + I\n",
    "    _adj = adj + sp.eye(adj.shape[0])\n",
    "    # _dseq：各个节点的度构成的列表\n",
    "    _dseq = _adj.sum(1).A1\n",
    "    # 构造开根号的度矩阵\n",
    "    _D_half = sp.diags(np.power(_dseq, -0.5))\n",
    "    # 计算标准化的邻接矩阵, @ 表示矩阵乘法\n",
    "    adj_normalized = _D_half @ _adj @ _D_half\n",
    "    return adj_normalized.tocsr()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 读取数据\n",
    "    # A_mat：邻接矩阵\n",
    "    # X_mat：特征矩阵\n",
    "    # z_vec：label\n",
    "    # train_idx,val_idx,test_idx: 要使用的节点序号\n",
    "    A_mat, X_mat, z_vec, train_idx, val_idx, test_idx = load_data_planetoid(FLAGS.dataset)\n",
    "    # 邻居矩阵标准化\n",
    "    An_mat = preprocess_graph(A_mat)\n",
    "\n",
    "    # 节点的类别个数\n",
    "    K = z_vec.max() + 1\n",
    "\n",
    "    # 构造GCN模型\n",
    "    gcn = GCN(An_mat, X_mat, [FLAGS.hidden1, K])\n",
    "    # 训练\n",
    "    gcn.train(train_idx, z_vec[train_idx], val_idx, z_vec[val_idx])\n",
    "    # 测试\n",
    "    test_res = gcn.evaluate(test_idx, z_vec[test_idx], training=False)\n",
    "    print(\"Dataset {}\".format(FLAGS.dataset),\n",
    "          \"Test loss {:.4f}\".format(test_res[0]),\n",
    "          \"test acc {:.4f}\".format(test_res[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
